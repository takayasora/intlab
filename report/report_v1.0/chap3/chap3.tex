\chapter{提案手法名をここに書く}
\thispagestyle{empty}
\label{chap3}
\minitoc

\newpage

\section{はじめに}

ここでは，論文を執筆するにあたり，マナーやルールを書いていく．

\clearpage
\newpage

\section{言葉遣い}\label{3 simulator}


基本的には「です・ます」調でかく．
レポートと基本的には同様で，「です・ます」調で書くと怒られる．

また，文学的な表現は避け，読者諸君とか吾輩はとか某は拙者は，とか書いてはいけない．


\clearpage
\newpage

\section{強化学習の構成}\label{3 RL system}
\ref{2 reinforcement learning}節で述べたように，強化学習は，ロボットが試行錯誤を繰り返し，最適な行動を学習していく枠組みである\cite{Sutton1998}\cite{木村1999}．
ロボットが確率的にある行動をとったときに，目的に合った行動をとると，報酬というスカラ量を得る．
学習を進めることで，ロボットは報酬を最大化する行動をとるようになる．
つまり強化学習を用いることにより，ロボットの目標に応じた報酬を与えることで環境に適応した動作を学習することができる．
本研究では強化学習の中でも多くの研究で用いられている$Q$学習を用いる．


\subsection{$Q$学習}\label{3 qlearning}
$Q$学習は初期状態から終端状態に至るまでの1エピソードの間に行動選択，価値関数の更新を繰り返し行う強化学習である．
$Q$学習では使われている方策とは独立に，行動価値関数$Q$を直接更新する．


\section{おわりに}\label{3 last}
本章では，リカバリモーション獲得の詳細について述べた．

\ref{3 simulator}節では，本研究で用いるシミュレータ内のロボットコントローラと強化学習器の関係について述べた．

\ref{3 RL system}節では，本研究で用いる強化学習のうち$Q$学習について述べた．
また，$Q$値に関してRBFネットワークによる近似を行うことを述べた．

\ref{3 reward function}節では，強化学習内で用いる報酬関数について述べた．
本研究で用いる報酬関数として移動ベクトルの報酬，目標到達の報酬を設定し，学習の促進を図る．
また，ロボットの移動中の安定性を評価するために，報酬関数にNE安定余裕の報酬を設定した．

次章では，動力学シミュレータを用いた実験を行う．

